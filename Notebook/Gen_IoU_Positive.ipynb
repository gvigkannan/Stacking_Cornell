{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook to evaluate and store the results from ggcnn for a sample imageset\n",
    "## Last Trial Run: 7th April 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import sys \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data \n",
    "import imageio\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggcnn_dir = \"/media/gvk/GVK_Elements/github_repo/ggcnn_stacking\"\n",
    "fadi_dir = \"/media/gvk/GVK_Elements/github_repo/Ensemble/Experts/Generative/pretrained_weights\"\n",
    "sys.path.append(ggcnn_dir)\n",
    "sys.path.append(fadi_dir)\n",
    "from models.common import post_process_output\n",
    "from utils.dataset_processing import evaluation, grasp \n",
    "from utils.data import get_dataset, cornell_data, cornell_data_neg\n",
    "from utils.data.cornell_data import CornellDataset\n",
    "from utils.data.cornell_data_neg import CornellDataset_Neg\n",
    "from utils.dataset_processing.image import DepthImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETERS:\n",
    "NO_GRASPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FadiColorNet_IoU_AllGrasps_FullCornell.csv\n"
     ]
    }
   ],
   "source": [
    "## Model Loading:\n",
    "MODEL_NAME = 'FadiColorNet'\n",
    "GRASP_TYPE = \"Positive\"\n",
    "MODEL_DIR = os.path.join(fadi_dir, \"color\", \"epoch_37_iou_0.80\")\n",
    "IOU_NAME = MODEL_NAME + \"_IoU_AllGrasps_FullCornell.csv\"\n",
    "depth_net = torch.load(MODEL_DIR)\n",
    "net = depth_net\n",
    "include_depth = False\n",
    "include_rgb = True\n",
    "print(IOU_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Considering one folder!\n",
    "dataset_path = \"/media/gvk/GVK_Elements/WPI/Spring2021/DR_Grasping/CurrentFocus/Cornell_Dataset/Full_Cornell/*/\"\n",
    "positive_data = CornellDataset(dataset_path, include_depth = False, include_rgb = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/media/gvk/GVK_Elements/WPI/Spring2021/DR_Grasping/CurrentFocus/Cornell_Dataset/Full_Cornell/data01/01/pcd0100cpos.txt'"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "positive_data.grasp_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "len(positive_data.depth_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the purpose of comparing images at a later stage.\n",
    "positive_comp = positive_data\n",
    "positive_data = torch.utils.data.DataLoader(positive_data, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, didx, rot, zoom = next(iter(positive_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cell Time:  108.09414744377136\n"
     ]
    }
   ],
   "source": [
    "## For all the images within the Cornell Dataset!\n",
    "\n",
    "results = {'correct': 0, 'failed': 0}\n",
    "start_cell = time.time()\n",
    "iou_list = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for img_idx, (x, y, didx, rot, zoom) in enumerate(positive_data):\n",
    "        xc = x.to(device)\n",
    "        yc = [yi.to(device) for yi in y]\n",
    "        lossd = net.compute_loss(xc, yc)    ## Ensure that your outputs are from a dataloader!\n",
    "\n",
    "        ## Getting the Quality, Angle and Width Images!\n",
    "        q_img, ang_img, width_img = post_process_output( lossd['pred']['pos'], lossd['pred']['cos'], lossd['pred']['sin'], lossd['pred']['width'])\n",
    "\n",
    "        ## Getting the ground truth Bounding Boxes for the particular image within Cornell Dataset - Refer dataset_preprocessing.evaluation.py for more details.\n",
    "        groundTruth_bbs = positive_data.dataset.get_gtbb(didx, rot, zoom)\n",
    "        if not isinstance(groundTruth_bbs, grasp.GraspRectangles):\n",
    "            groundTruth_bbs = grasp.GraspRectangles.load_from_array(groundTruth_bbs)\n",
    "        \n",
    "        grasp_predict = grasp.detect_grasps(q_img, ang_img, width_img, no_grasps = NO_GRASPS)\n",
    "        \n",
    "        iou_dict = {}\n",
    "        ## For each grasp available:\n",
    "        for g_idx, model_grasp in enumerate(grasp_predict):\n",
    "            for gt_idx, gt_bb in enumerate(groundTruth_bbs):\n",
    "                ## Calculate IoU:\n",
    "                grasp_predict = model_grasp.as_gr\n",
    "\n",
    "                key_name = \"_\".join((\"ModelGrasp\", str(g_idx), \"GT_Grasp\", str(gt_idx)))\n",
    "                ## Detecting the IoU between the Grasp and Ground Truth Bounding Box\n",
    "                iou_dict[key_name] = grasp_predict.iou(gt_bb)\n",
    "        \n",
    "        ## Saving the IoU generated!\n",
    "        ## Just getting the pcd name:\n",
    "        \n",
    "        img_id = positive_comp.depth_files[img_idx].split(\"/\")[-1]  ## Now its pcdxxxx.tiff\n",
    "        img_id = str(img_id).split(\".\")[0][:-1]                 ## Now its pcdxxxx\n",
    "        for key_grasp, value_grasp in iou_dict.items():\n",
    "            grasp_result = value_grasp > 0.25\n",
    "            iou_list.append([img_id, key_grasp, value_grasp, grasp_result, didx.item(), zoom.item(), rot.item()])\n",
    "print(\"Cell Time: \", time.time() - start_cell) ## ~52s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['pcd0100', 'ModelGrasp_0_GT_Grasp_0', 0.2835249042145594, True, 0, 1.0, 0.0]"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "iou_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Image_ID                 Grasp_ID       IoU  Grasp_Result  Didx_Image  \\\n",
       "0     pcd0100  ModelGrasp_0_GT_Grasp_0  0.283525          True           0   \n",
       "1     pcd0100  ModelGrasp_0_GT_Grasp_1  0.448869          True           0   \n",
       "2     pcd0100  ModelGrasp_0_GT_Grasp_2  0.048695         False           0   \n",
       "3     pcd0100  ModelGrasp_0_GT_Grasp_3  0.000000         False           0   \n",
       "4     pcd0101  ModelGrasp_0_GT_Grasp_0  0.000000         False           1   \n",
       "...       ...                      ...       ...           ...         ...   \n",
       "5105  pcd1034  ModelGrasp_0_GT_Grasp_0  0.000000         False         884   \n",
       "5106  pcd1034  ModelGrasp_0_GT_Grasp_1  0.052144         False         884   \n",
       "5107  pcd1034  ModelGrasp_0_GT_Grasp_2  0.000000         False         884   \n",
       "5108  pcd1034  ModelGrasp_0_GT_Grasp_3  0.336987          True         884   \n",
       "5109  pcd1034  ModelGrasp_0_GT_Grasp_4  0.409531          True         884   \n",
       "\n",
       "      Image_Zoom  Image_Rot  Grasp_Type  \n",
       "0            1.0        0.0           1  \n",
       "1            1.0        0.0           1  \n",
       "2            1.0        0.0           1  \n",
       "3            1.0        0.0           1  \n",
       "4            1.0        0.0           1  \n",
       "...          ...        ...         ...  \n",
       "5105         1.0        0.0           1  \n",
       "5106         1.0        0.0           1  \n",
       "5107         1.0        0.0           1  \n",
       "5108         1.0        0.0           1  \n",
       "5109         1.0        0.0           1  \n",
       "\n",
       "[5110 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>Grasp_ID</th>\n      <th>IoU</th>\n      <th>Grasp_Result</th>\n      <th>Didx_Image</th>\n      <th>Image_Zoom</th>\n      <th>Image_Rot</th>\n      <th>Grasp_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pcd0100</td>\n      <td>ModelGrasp_0_GT_Grasp_0</td>\n      <td>0.283525</td>\n      <td>True</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pcd0100</td>\n      <td>ModelGrasp_0_GT_Grasp_1</td>\n      <td>0.448869</td>\n      <td>True</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pcd0100</td>\n      <td>ModelGrasp_0_GT_Grasp_2</td>\n      <td>0.048695</td>\n      <td>False</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pcd0100</td>\n      <td>ModelGrasp_0_GT_Grasp_3</td>\n      <td>0.000000</td>\n      <td>False</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pcd0101</td>\n      <td>ModelGrasp_0_GT_Grasp_0</td>\n      <td>0.000000</td>\n      <td>False</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5105</th>\n      <td>pcd1034</td>\n      <td>ModelGrasp_0_GT_Grasp_0</td>\n      <td>0.000000</td>\n      <td>False</td>\n      <td>884</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5106</th>\n      <td>pcd1034</td>\n      <td>ModelGrasp_0_GT_Grasp_1</td>\n      <td>0.052144</td>\n      <td>False</td>\n      <td>884</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5107</th>\n      <td>pcd1034</td>\n      <td>ModelGrasp_0_GT_Grasp_2</td>\n      <td>0.000000</td>\n      <td>False</td>\n      <td>884</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5108</th>\n      <td>pcd1034</td>\n      <td>ModelGrasp_0_GT_Grasp_3</td>\n      <td>0.336987</td>\n      <td>True</td>\n      <td>884</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5109</th>\n      <td>pcd1034</td>\n      <td>ModelGrasp_0_GT_Grasp_4</td>\n      <td>0.409531</td>\n      <td>True</td>\n      <td>884</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5110 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "iou_pd = pd.DataFrame(iou_list)\n",
    "iou_pd.columns = ['Image_ID', 'Grasp_ID', 'IoU', 'Grasp_Result', 'Didx_Image', 'Image_Zoom', 'Image_Rot']\n",
    "iou_pd['Grasp_Type'] = +1\n",
    "iou_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iou_pd = pd.DataFrame(iou_list)\n",
    "# iou_pd.columns = ['Image_ID', 'Grasp_ID', 'IoU', 'Grasp_Result', 'Didx_Image', 'Image_Zoom', 'Image_Rot']\n",
    "# iou_pd['Grasp_Type'] = +1\n",
    "# iou_pd"
   ]
  },
  {
   "source": [
    "iou_pd.to_csv(IOU_NAME, sep = ',', index = False)\n",
    "IOU_NAME"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 133,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'FadiColorNet_IoU_AllGrasps_FullCornell.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ]
  },
  {
   "source": [
    "STOP"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 134,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-ee4decdca3b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSTOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(iou_quality.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## To get the image id\n",
    "# img_id = test_comp.rgb_files[1].split(\"/\")[-1]\n",
    "# depth_id = str(img_id).split('.')[0][:-1]\n",
    "# print(depth_id)\n",
    "\n",
    "# rgb_id = str(img_id).split('.')[0][:-1]\n",
    "# print(rgb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Saving the Collected Results!\n",
    "# import pickle\n",
    "# model_name = \"fadi_depth\"\n",
    "# save_dir = \"/media/gvk/GVK_Elements/github_repo/Ensemble/Stacking_Test/Stacking_Data/\"\n",
    "# first_entry, last_entry = list(iou_quality.keys())[0], list(iou_quality.keys())[-1]\n",
    "# fname = \"_\".join((model_name, first_entry, last_entry+\".txt\"))\n",
    "# fdir = os.path.join(save_dir, fname)\n",
    "# with open(fdir, 'wb') as file_pickle:\n",
    "#     pickle.dump(iou_quality, file_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cell Time:  0.06836748123168945\n"
     ]
    }
   ],
   "source": [
    "#  Trial for each test image\n",
    "## Test Worked: 11th May 2021\n",
    "## To detect whether the grasp worked for all possible grasps!\n",
    "start_cell = time.time()\n",
    "results = {'correct': 0, 'failed': 0}\n",
    "iou_quality = {}\n",
    "with torch.no_grad():\n",
    "\n",
    "    ## For each test image\n",
    "    ## Just testing for one single image\n",
    "    x, y, didx, rot, zoom =  next(iter(positive_data))\n",
    "    img_idx = 0\n",
    "    xc = x.to(device)\n",
    "    yc = [yi.to(device) for yi in y]\n",
    "    lossd = net.compute_loss(xc, yc)    ## Ensure that your outputs are from a dataloader!\n",
    "\n",
    "    ## Getting the Quality, Angle and Width Images!\n",
    "    q_img, ang_img, width_img = post_process_output( lossd['pred']['pos'], lossd['pred']['cos'], lossd['pred']['sin'], lossd['pred']['width'])\n",
    "\n",
    "    ## Getting the ground truth Bounding Boxes for the particular image within Cornell Dataset - Refer dataset_preprocessing.evaluation.py for more details.\n",
    "    groundTruth_bbs = positive_data.dataset.get_gtbb(didx, rot, zoom)\n",
    "    if not isinstance(groundTruth_bbs, grasp.GraspRectangles):\n",
    "        groundTruth_bbs = grasp.GraspRectangles.load_from_array(groundTruth_bbs)\n",
    "    \n",
    "    grasp_predict = grasp.detect_grasps(q_img, ang_img, width_img, no_grasps = NO_GRASPS)\n",
    "    \n",
    "    iou_dict = {}\n",
    "    ## For each grasp available:\n",
    "    for g_idx, model_grasp in enumerate(grasp_predict):\n",
    "        for gt_idx, gt_bb in enumerate(groundTruth_bbs):\n",
    "            ## Calculate IoU:\n",
    "            grasp_predict = model_grasp.as_gr\n",
    "\n",
    "            key_name = \"_\".join((\"ModelGrasp\", str(g_idx), \"GT_Grasp\", str(gt_idx)))\n",
    "            ## Detecting the IoU between the Grasp and Ground Truth Bounding Box\n",
    "            iou_dict[key_name] = grasp_predict.iou(gt_bb)\n",
    "    \n",
    "    ## Saving the IoU generated!\n",
    "    ## Just getting the pcd name:\n",
    "    iou_list = []\n",
    "    img_id = positive_comp.depth_files[img_idx].split(\"/\")[-1]  ## Now its pcdxxxx.tiff\n",
    "    img_id = str(img_id).split(\".\")[0][:-1]                 ## Now its pcdxxxx\n",
    "    for key_grasp, value_grasp in iou_dict.items():\n",
    "        grasp_result = value_grasp > 0.25\n",
    "        iou_list.append([img_id, key_grasp, value_grasp, grasp_result, didx.item(), zoom.item(), rot.item()])\n",
    "\n",
    "\n",
    "    # ## Now to compare and create a dictionary that stores the IoU given image name\n",
    "    # x_comp, _, _, _, _ = test_comp[idx]\n",
    "    # #print(torch.allclose(torch.tensor(x_comp), x))\n",
    "    # if torch.allclose(torch.tensor(x_comp), x):\n",
    "    #     img_id = test_comp.depth_files[0].split(\"/\")[-1]\n",
    "\n",
    "    #     ## We cannot use an image!\n",
    "    #     #quality_dict[str(img_id)] = q_img\n",
    "    #     image_id = str(img_id).split('.')[0][:-1]\n",
    "\n",
    "    #     # iou_quality[image_id] = [int(s), np.max(q_img).astype('float32')]\n",
    "    # else:\n",
    "    #     print(\"Dataloader and Comparison X didn't match\")\n",
    "\n",
    "print(\"Cell Time: \", time.time() - start_cell)"
   ]
  }
 ]
}