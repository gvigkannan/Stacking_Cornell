{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook to evaluate and store the results from ggcnn for a sample imageset\n",
    "## Last Trial Run: 7th April 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import sys \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data \n",
    "import imageio\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggcnn_dir = \"/media/gvk/GVK_Elements/WPI/Spring2021/DR_Grasping/CurrentFocus/ggcnn_repo/\"\n",
    "fadi_dir = \"/media/gvk/GVK_Elements/github_repo/Ensemble/Experts/Generative/pretrained_weights\"\n",
    "sys.path.append(ggcnn_dir)\n",
    "sys.path.append(fadi_dir)\n",
    "from models.common import post_process_output\n",
    "from utils.dataset_processing import evaluation, grasp \n",
    "from utils.data import get_dataset\n",
    "from utils.dataset_processing.image import DepthImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Search Path:  /media/gvk/GVK_Elements/WPI/Spring2021/DR_Grasping/CurrentFocus/Cornell_Dataset/Full_Cornell/*/*/pcd*cpos.txt\n"
     ]
    }
   ],
   "source": [
    "## Considering one folder!\n",
    "dataset_path = \"/media/gvk/GVK_Elements/WPI/Spring2021/DR_Grasping/CurrentFocus/Cornell_Dataset/Full_Cornell/*/\"\n",
    "dataset = get_dataset(\"cornell\")\n",
    "sys.path.insert(0, dataset_path)\n",
    "test_data = dataset(dataset_path, include_depth = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Search Path:  /media/gvk/GVK_Elements/WPI/Spring2021/DR_Grasping/CurrentFocus/Cornell_Dataset/Full_Cornell/data01/*/pcd*cpos.txt\n"
     ]
    }
   ],
   "source": [
    "# ## Considering one folder!\n",
    "# folder1_path = \"/media/gvk/GVK_Elements/WPI/Spring2021/DR_Grasping/CurrentFocus/Cornell_Dataset/Full_Cornell/data01/\"\n",
    "# dataset = get_dataset(\"cornell\")\n",
    "# sys.path.insert(0, folder1_path)\n",
    "# test_data = dataset(folder1_path, include_depth = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(test_data.depth_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "/media/gvk/GVK_Elements/py_linux/ensemble_dr_linux/lib/python3.8/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/media/gvk/GVK_Elements/py_linux/ensemble_dr_linux/lib/python3.8/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/media/gvk/GVK_Elements/py_linux/ensemble_dr_linux/lib/python3.8/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/media/gvk/GVK_Elements/py_linux/ensemble_dr_linux/lib/python3.8/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/media/gvk/GVK_Elements/py_linux/ensemble_dr_linux/lib/python3.8/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.upsampling.UpsamplingBilinear2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "## Model Loading:\n",
    "%time\n",
    "ggcnn2_model_dir = \"/media/gvk/GVK_Elements/WPI/Spring2021/DR_Grasping/CurrentFocus/ggcnn_repo/weights/ggcnn2_weights_cornell/epoch_50_cornell\"\n",
    "ggcnn2_net = torch.load(ggcnn2_model_dir)\n",
    "net = ggcnn2_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comp = test_data\n",
    "test_data = torch.utils.data.DataLoader(test_data, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cell Time:  0.41224050521850586\n"
     ]
    }
   ],
   "source": [
    "# ##  Trial for each test image\n",
    "# start_cell = time.time()\n",
    "# results = {'correct': 0, 'failed': 0}\n",
    "# iou_quality = {}\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     ## For each test image\n",
    "#     ## Just testing for one single image\n",
    "#     x, y, didx, rot, zoom =  next(iter(test_data))\n",
    "#     idx = 0\n",
    "#     xc = x.to(device)\n",
    "#     yc = [yi.to(device) for yi in y]\n",
    "#     lossd = net.compute_loss(xc, yc)    ## Ensure that your outputs are from a dataloader!\n",
    "\n",
    "#     q_img, ang_img, width_img = post_process_output( lossd['pred']['pos'], lossd['pred']['cos'], lossd['pred']['sin'], lossd['pred']['width'])\n",
    "#     s = evaluation.calculate_iou_match(q_img, ang_img, test_data.dataset.get_gtbb(didx, rot, zoom), no_grasps = 1, grasp_width = width_img)\n",
    "\n",
    "#     if s:\n",
    "#         results['correct'] += 1\n",
    "#     else:\n",
    "#         results['failed'] += 1\n",
    "\n",
    "#     iou_dict = {}\n",
    "#     ## Now to compare and create a dictionary that stores the quality given image name\n",
    "#     x_comp, _, _, _, _ = test_comp[idx]\n",
    "#     #print(torch.allclose(torch.tensor(x_comp), x))\n",
    "#     if torch.allclose(torch.tensor(x_comp), x):\n",
    "#         img_id = test_comp.depth_files[0].split(\"/\")[-1]\n",
    "\n",
    "#         ## We cannot use an image!\n",
    "#         #quality_dict[str(img_id)] = q_img\n",
    "#         image_id = str(img_id).split('.')[0][:-1]\n",
    "#         iou_quality[image_id] = [int(s), np.max(q_img).astype('float32')]\n",
    "#     else:\n",
    "#         print(\"Dataloader and Comparison X didn't match\")\n",
    "\n",
    "# print(\"Cell Time: \", time.time() - start_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cell Time:  89.68731474876404\n"
     ]
    }
   ],
   "source": [
    "results = {'correct': 0, 'failed': 0}\n",
    "start_cell = time.time()\n",
    "iou_quality = {}\n",
    "with torch.no_grad():\n",
    "\n",
    "\n",
    "    for idx, (x, y, didx, rot, zoom) in enumerate(test_data):\n",
    "        xc = x.to(device)\n",
    "        yc = [yi.to(device) for yi in y]\n",
    "        lossd = net.compute_loss(xc, yc)\n",
    "\n",
    "        q_img, ang_img, width_img = post_process_output( lossd['pred']['pos'], lossd['pred']['cos'], lossd['pred']['sin'], lossd['pred']['width'])\n",
    "        s = evaluation.calculate_iou_match(q_img, ang_img, test_data.dataset.get_gtbb(didx, rot, zoom), no_grasps = 1, grasp_width = width_img)\n",
    "\n",
    "        if s:\n",
    "            results['correct'] += 1\n",
    "        else:\n",
    "            results['failed'] += 1\n",
    "\n",
    "        ## Need to store: q_img and img_id\n",
    "        ## Now to compare and create a dictionary that stores the quality given image name\n",
    "        x_comp, _, _, _, _ = test_comp[idx]\n",
    "        \n",
    "        ## Now, trying to \n",
    "        if torch.allclose(torch.tensor(x_comp), x):\n",
    "            img_id = test_comp.depth_files[idx].split(\"/\")[-1]\n",
    "            img_id = str(img_id).split('.')[0][:-1]\n",
    "            ## We cannot use an image!\n",
    "            #quality_dict[str(img_id)] = q_img\n",
    "            iou_quality[str(img_id)] = [int(s), np.max(q_img).astype('float32')]\n",
    "        else:\n",
    "            print(\"Dataloader and Comparison X didn't match\")\n",
    "print(\"Cell Time: \", time.time() - start_cell) ## ~90s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'pcd0100': [1, 0.78971946], 'pcd0101': [1, 0.8981491], 'pcd0102': [1, 0.7706125], 'pcd0103': [1, 0.9016831], 'pcd0104': [1, 0.7905614], 'pcd0105': [0, 0.85269386], 'pcd0106': [0, 0.12784792], 'pcd0107': [0, 0.07468596], 'pcd0108': [0, 0.0750214], 'pcd0109': [0, 0.0789577], 'pcd0110': [0, 0.952965], 'pcd0111': [1, 0.90971625], 'pcd0112': [0, 0.9798054], 'pcd0113': [0, 0.84903944], 'pcd0114': [0, 0.8830497], 'pcd0115': [0, 0.9404322], 'pcd0116': [1, 0.9564643], 'pcd0117': [1, 1.2627996], 'pcd0118': [0, 0.07337329], 'pcd0119': [1, 0.52589566], 'pcd0120': [1, 0.62857115], 'pcd0121': [0, 0.6219607], 'pcd0122': [1, 0.9120887], 'pcd0123': [0, 0.87171906], 'pcd0124': [1, 0.8176524], 'pcd0125': [0, 0.8733429], 'pcd0126': [0, 0.6590582], 'pcd0127': [0, 0.88319856], 'pcd0128': [0, 0.8827105], 'pcd0129': [0, 0.07505435], 'pcd0130': [0, 0.082016736], 'pcd0131': [0, 0.0959656], 'pcd0132': [0, 0.2175512], 'pcd0133': [0, 0.07445402], 'pcd0134': [0, 0.07422224], 'pcd0135': [0, 0.07516372], 'pcd0136': [0, 0.7004124], 'pcd0137': [1, 0.88175523], 'pcd0138': [1, 0.9339751], 'pcd0139': [0, 0.93874675], 'pcd0140': [0, 1.0298383], 'pcd0141': [1, 1.2272959], 'pcd0142': [1, 1.0024256], 'pcd0143': [1, 1.4109226], 'pcd0144': [1, 1.0251893], 'pcd0145': [1, 0.8716277], 'pcd0146': [0, 1.4858637], 'pcd0147': [1, 1.1651142], 'pcd0148': [1, 1.0338191], 'pcd0149': [1, 0.9735793], 'pcd0150': [0, 0.94442934], 'pcd0151': [0, 1.0779637], 'pcd0152': [0, 0.9533868], 'pcd0153': [0, 0.07432692], 'pcd0154': [0, 0.07472663], 'pcd0155': [0, 0.07425302], 'pcd0156': [0, 0.074296504], 'pcd0157': [0, 0.07219431], 'pcd0158': [1, 0.8244412], 'pcd0159': [0, 0.868497], 'pcd0160': [0, 0.7873676], 'pcd0161': [1, 0.90162444], 'pcd0162': [1, 0.91424936], 'pcd0163': [1, 0.7908611], 'pcd0164': [0, 0.95547533], 'pcd0165': [0, 0.87434834], 'pcd0166': [0, 0.77937555], 'pcd0167': [0, 0.87881285], 'pcd0168': [1, 0.8960699], 'pcd0169': [1, 0.91567093], 'pcd0170': [1, 0.7703654], 'pcd0171': [1, 0.9034016], 'pcd0172': [1, 0.8780211], 'pcd0173': [1, 0.2760473], 'pcd0174': [0, 0.32258716], 'pcd0175': [0, 0.2690424], 'pcd0176': [0, 0.29338983], 'pcd0177': [0, 0.09679388], 'pcd0178': [0, 0.074301295], 'pcd0179': [0, 0.11347398], 'pcd0180': [0, 0.07428096], 'pcd0181': [1, 0.83981246], 'pcd0182': [1, 0.6873315], 'pcd0183': [1, 0.838619], 'pcd0184': [1, 0.8123248], 'pcd0185': [0, 0.07449876], 'pcd0186': [0, 0.07503357], 'pcd0187': [0, 0.09235209], 'pcd0188': [0, 0.074843526], 'pcd0189': [0, 0.38413775], 'pcd0190': [0, 0.07376763], 'pcd0191': [0, 0.7557687], 'pcd0192': [1, 0.41871428], 'pcd0193': [0, 0.07515583], 'pcd0194': [0, 0.075331554], 'pcd0195': [0, 0.1865579], 'pcd0196': [0, 0.5008191], 'pcd0197': [1, 0.8505542], 'pcd0198': [0, 0.8343534], 'pcd0199': [0, 0.9372087]}\n"
     ]
    }
   ],
   "source": [
    "# print(iou_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pcd0101\npcd0101\n"
     ]
    }
   ],
   "source": [
    "## To get the image id\n",
    "img_id = test_comp.rgb_files[1].split(\"/\")[-1]\n",
    "depth_id = str(img_id).split('.')[0][:-1]\n",
    "print(depth_id)\n",
    "\n",
    "rgb_id = str(img_id).split('.')[0][:-1]\n",
    "print(rgb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the Data Collected\n",
    "import pickle\n",
    "model_name = \"ggcnn2_depth\"\n",
    "save_dir = \"/media/gvk/GVK_Elements/github_repo/Ensemble/Stacking_Test/Stacking_Data/\"\n",
    "first_entry, last_entry = list(iou_quality.keys())[0], list(iou_quality.keys())[-1]\n",
    "fname = \"_\".join((model_name, first_entry, last_entry+\".txt\"))\n",
    "fdir = os.path.join(save_dir, fname)\n",
    "with open(fdir, 'wb') as file_pickle:\n",
    "    pickle.dump(iou_quality, file_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/media/gvk/GVK_Elements/github_repo/Ensemble/Stacking_Test/Stacking_Data/ggcnn2_depth_pcd0100_pcd1034.txt'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "fdir"
   ]
  }
 ]
}